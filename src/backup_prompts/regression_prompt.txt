You are provided with the following Python regression model:

{current_model_code}

Regression metrics for this model are:
{metrics_str}

Previous regression models and their performance metrics are:
{history_str}

Additional Information:
{extra_info}

Task:
Based on the given regression model and its performance, suggest improvements. You may either:
    - Change the regression model to a different one (e.g., XGBRegressor).
    - Adjust the hyperparameters of the current model, especially if the metrics are already strong.
    - Propose a custom loss function to address the additional information (e.g., noisy data, outliers, heteroscedasticity).

**Example 1** (Switching to a more powerful model):
Previous Model:
def load_model():
    from sklearn.linear_model import LinearRegression
    return LinearRegression()

Metrics:
Mean Squared Error: 250.75
R^2 Score: 0.85

Extra Info:
Not available

Suggested Improvement:
Since the performance is decent but could be improved, switch to a more powerful model like XGBRegressor:
def load_model():
    from xgboost import XGBRegressor
    return XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)

**Example 2** (Outliers in the Data):
Previous Model:
def load_model():
    from sklearn.ensemble import RandomForestRegressor
    return RandomForestRegressor(n_estimators=500, max_depth=50)

Metrics:
Mean Squared Error: 200.5
R^2 Score: 0.90

Extra Info:
The dataset contains some extreme outliers that are affecting the performance of the model.

Suggested Improvement:
Since there are outliers, use XGBRegressor with Huber loss to reduce the influence of extreme values on the modelâ€™s performance:
def load_model():
    from xgboost import XGBRegressor
    return XGBRegressor(n_estimators=500, max_depth=10, learning_rate=0.05, objective='reg:squaredlogerror')

Explanation:
Huber loss is a robust loss function that combines the benefits of L2 loss for small errors and L1 loss for large errors. This makes the model more robust to outliers in the dataset.

**Example 3** (Heteroscedasticity in the Data):
Previous Model:
def load_model():
    from sklearn.ensemble import RandomForestRegressor
    return RandomForestRegressor(n_estimators=700, max_depth=60, min_samples_split=5)

Metrics:
Mean Squared Error: 180.0
R^2 Score: 0.92

Extra Info:
The residuals show signs of heteroscedasticity (i.e., non-constant variance in the errors).

Suggested Improvement:
Use XGBRegressor with a custom objective function to model heteroscedasticity:
def load_model():
    from xgboost import XGBRegressor
    return XGBRegressor(n_estimators=600, max_depth=8, learning_rate=0.1, objective='reg:pseudohubererror')

Explanation:
The pseudohuber error is effective for handling datasets with heteroscedasticity. It balances between mean absolute error (L1) and mean squared error (L2), allowing the model to handle non-constant variance in the residuals.

**Example 4** (Noisy Data):
Previous Model:
def load_model():
    from sklearn.linear_model import Ridge
    return Ridge(alpha=1.0)

Metrics:
Mean Squared Error: 300.0
R^2 Score: 0.80

Extra Info:
There is suspected noise in the input features, making the model less accurate.

Suggested Improvement:
Switch to a model that is more robust to noise, like XGBRegressor, and add regularization to avoid overfitting to the noisy data:
def load_model():
    from xgboost import XGBRegressor
    return XGBRegressor(n_estimators=800, max_depth=6, learning_rate=0.03, reg_lambda=10.0, reg_alpha=1.0)

Explanation:
Increasing both `reg_lambda` and `reg_alpha` adds regularization to the model, helping to mitigate the impact of noisy input features and reducing overfitting.

**Example 5** (Bias-Variance Tradeoff):
Previous Model:
def load_model():
    from sklearn.ensemble import RandomForestRegressor
    return RandomForestRegressor(n_estimators=600, max_depth=12, min_samples_split=4)

Metrics:
Mean Squared Error: 150.0
R^2 Score: 0.94

Extra Info:
The model shows signs of high variance (overfitting) to the training data but performs well on test data.

Suggested Improvement:
Reduce the complexity of the model by adjusting hyperparameters to achieve a better balance between bias and variance:
def load_model():
    from xgboost import XGBRegressor
    return XGBRegressor(n_estimators=400, max_depth=6, learning_rate=0.1, subsample=0.7, colsample_bytree=0.7)

Explanation:
Reducing the number of trees (`n_estimators=400`), shrinking the depth of the trees (`max_depth=6`), and lowering the `subsample` and `colsample_bytree` values will help reduce overfitting by controlling model complexity and improving the bias-variance tradeoff.

Please ensure all necessary imports are included within the function.
Provide only executable Python code for the improved regression model without any comments, explanations, or markdown formatting.

Output:
Provide only the improved Python code that can replace the current model.

