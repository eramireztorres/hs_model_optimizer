You are provided with the following Python model that implements a machine learning classifier:

{current_model_code}

Classification metrics for this model are:
{metrics_str}

Previous models and their performance metrics are:
{history_str}

Additional Information:
{extra_info}

Task:
Based on the given model and its performance, suggest improvements. You may either:
    - Change the model to a different classifier (e.g., XGBoost).
    - Adjust the hyperparameters of the current model, especially if the metrics are already high.
    - Propose a custom loss function to address the additional information (e.g., class imbalance, noisy labels).
    
**Example 1** (Strong Metrics, Small Hyperparameter Tuning):
Previous Model:
def load_model():
    from sklearn.ensemble import ExtraTreesClassifier
    return ExtraTreesClassifier(n_estimators=1200, max_depth=60, min_samples_split=2, min_samples_leaf=1)

Metrics:
Accuracy: 0.936
Precision: 0.937
Recall: 0.936
F1 Score: 0.936

Extra Info:
Not available

Suggested Improvement:
Since the metrics are strong, a small adjustment in hyperparameters:
def load_model():
    from sklearn.ensemble import ExtraTreesClassifier
    return ExtraTreesClassifier(n_estimators=1500, max_depth=70, min_samples_split=2, min_samples_leaf=1)

**Example 2** (Switch to XGBoost for Improved Performance):
Previous Model:
def load_model():
    from sklearn.ensemble import RandomForestClassifier
    return RandomForestClassifier(n_estimators=800, max_depth=50, min_samples_split=2, min_samples_leaf=1)

Metrics:
Accuracy: 0.92
Precision: 0.92
Recall: 0.92
F1 Score: 0.92

Extra Info:
Not available

Suggested Improvement:
Switch to a more powerful model like XGBoost for improved performance:
def load_model():
    from xgboost import XGBClassifier
    return XGBClassifier(n_estimators=500, max_depth=10, learning_rate=0.05, subsample=0.8)

**Example 3** (Class Imbalance):
Previous Model:
def load_model():
    from sklearn.ensemble import ExtraTreesClassifier
    return ExtraTreesClassifier(n_estimators=1200, max_depth=60, min_samples_split=2, min_samples_leaf=1)

Metrics:
Accuracy: 0.936
Precision: 0.937
Recall: 0.936
F1 Score: 0.936

Extra Info:
Binary classification problem with a class imbalance: there are 4 times more instances of class 0 than class 1.

Suggested Improvement:
Given the class imbalance, we suggest using XGBoost with a custom `scale_pos_weight` parameter to penalize the majority class (class 0) and better handle the minority class (class 1):
def load_model():
    from xgboost import XGBClassifier
    return XGBClassifier(n_estimators=1000, max_depth=10, learning_rate=0.1, scale_pos_weight=4)

Explanation:
The `scale_pos_weight=4` parameter adjusts the model's sensitivity to class 1 by weighting its misclassification more heavily. This will help address the class imbalance and improve recall for the minority class.

**Example 4** (Noisy Labels):
Previous Model:
def load_model():
    from sklearn.ensemble import RandomForestClassifier
    return RandomForestClassifier(n_estimators=800, max_depth=50, min_samples_split=2, min_samples_leaf=1)

Metrics:
Accuracy: 0.92
Precision: 0.92
Recall: 0.92
F1 Score: 0.92

Extra Info:
There are doubts about the accuracy of some class labels. It is suspected that up to 15% of the labels might be noisy or incorrect.

Suggested Improvement:
Use XGBoost with a custom loss function to reduce the impact of noisy labels on the modelâ€™s performance:
def load_model():
    from xgboost import XGBClassifier
    return XGBClassifier(n_estimators=800, max_depth=10, learning_rate=0.1, gamma=0.5)

Explanation:
Using `gamma=0.5` in XGBoost increases regularization, making the model more robust to noisy labels. It helps by preventing overfitting to potentially mislabeled instances.

**Example 5** (Class Imbalance and Noisy Labels):
Previous Model:
def load_model():
    from sklearn.ensemble import ExtraTreesClassifier
    return ExtraTreesClassifier(n_estimators=1000, max_depth=40)

Metrics:
Accuracy: 0.89
Precision: 0.88
Recall: 0.85
F1 Score: 0.86

Extra Info:
Class imbalance is present, with class 0 being 3 times more frequent than class 1. Additionally, there are concerns about the accuracy of some labels.

Suggested Improvement:
Use XGBoost with both a custom loss function to handle noisy labels and `scale_pos_weight` to account for class imbalance:
def load_model():
    from xgboost import XGBClassifier
    return XGBClassifier(n_estimators=500, max_depth=15, learning_rate=0.05, scale_pos_weight=3, eval_metric='auc', gamma=0.5)

Explanation:
This approach combines class balancing via `scale_pos_weight=3` and robustness to noisy labels via `gamma=0.5`. The model will better handle both issues simultaneously.

Please ensure all necessary imports are included within the function.
Provide only executable Python code for the improved model without any comments, explanations, or markdown formatting.

Output:
Provide only the improved Python code that can replace the current model.


